import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# 1. Inicializar el modelo de embeddings (el mismo que us√°bamos)
# Este modelo convierte texto en una lista de n√∫meros (vectores)
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# 2. Configurar el √≠ndice de FAISS
# '384' es la dimensi√≥n de los vectores que genera el modelo MiniLM
dimension = 384
index = faiss.IndexFlatL2(dimension)

# Memoria persistente simple para guardar el texto original
memoria_texto = []

def guardar_recuerdo(texto):
    """ Convierte texto a vector y lo guarda en el √≠ndice """
    # Convertimos el texto a vector
    vector = embedder.encode([texto])
    # Agregamos al √≠ndice de FAISS
    index.add(np.array(vector).astype('float32'))
    # Guardamos el texto para poder recuperarlo luego
    memoria_texto.append(texto)
    print(f"‚úÖ Recuerdo guardado: {texto[:50]}...")

def recuperar_recuerdos(consulta, n_resultados=1):
    """ Busca los recuerdos m√°s cercanos vectorialmente """
    vector_consulta = embedder.encode([consulta])
    # FAISS busca los 'n' vectores m√°s parecidos
    distancias, indices = index.search(np.array(vector_consulta).astype('float32'), n_resultados)
    
    resultados = []
    for i in indices[0]:
        if i != -1: # -1 significa que no encontr√≥ nada
            resultados.append(memoria_texto[i])
    return resultados

# --- Prueba en vivo ---
if __name__ == "__main__":
    guardar_recuerdo("El cliente prefiere auditor√≠as con Nemesis IA los lunes.")
    
    print("\nüîç Consultando memoria...")
    recuerdos = recuperar_recuerdos("¬øCu√°ndo prefiere el cliente las auditor√≠as?")
    print(f"ü§ñ El agente recuerda: {recuerdos}")
from openai import OpenAI
import logging

# Configuraci√≥n de logs para observabilidad Enterprise
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio"
)

class Orquestador:
    def __init__(self, memoria, seguridad, auditor):
        self.memoria = memoria
        self.seguridad = seguridad
        self.auditor = auditor
        self.max_tokens_contexto = 6000 # L√≠mite para qwen2-7b 

    def gestionar_ventana_contexto(self, contexto):
        """ Hardening: Recorta el contexto si excede el l√≠mite de la ventana """
        if len(str(contexto)) > self.max_tokens_contexto:
            logging.warning("‚ö†Ô∏è Contexto excedido. Aplicando recorte (Trimming).")
            return contexto[-self.max_tokens_contexto:]
        return contexto

    def procesar(self, mensaje):
        # 1. Recuperaci√≥n y limpieza de contexto (RAG) [cite: 8732]
        contexto_crudo = self.memoria.recuperar(mensaje)
        contexto_limpio = self.gestionar_ventana_contexto(contexto_crudo)
        
        # 2. Validaci√≥n de Seguridad (Protocolo N√©mesis) [cite: 9682]
        if not self.seguridad.validar(mensaje):
            logging.error(f"‚ùå Intento de inyecci√≥n detectado en: {mensaje}")
            return "Error: Petici√≥n bloqueada por pol√≠ticas de seguridad."

        # 3. Ejecuci√≥n del Agente
        respuesta = self._ejecutar_agente(mensaje, contexto_limpio)
        
        # 4. Auditor√≠a y Registro [cite: 9110]
        self.auditor.registrar(mensaje, respuesta)
        return respuesta

    def _ejecutar_agente(self, mensaje, contexto):
        try:
            # Correcci√≥n de sintaxis y versionado de modelo [cite: 8462]
            response = client.chat.completions.create(
                model="qwen2-7b-instruct", 
                messages=[
                    {"role": "system", "content": f"Contexto de Memoria Soberana: {contexto}"},
                    {"role": "user", "content": mensaje}
                ],
                temperature=0.2 # Rigor t√©cnico solicitado [cite: 9049]
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"‚ùå Error en inferencia: {e}")
            return "Lo siento, el cerebro local no responde."

# Nota: Este c√≥digo ahora cumple con la gesti√≥n de errores y el logging
# que exige un entorno de producci√≥n real para 2026.

# --- 3. ENTRADA PRINCIPAL (MAIN) ---
if __name__ == "__main__":
    # 1. Simulaci√≥n de los Componentes (Mocks para la prueba)
    class MemoriaMock:
        def recuperar(self, msg): 
            return "DATO_TECNICO: El servidor de Vertex usa el puerto 8080."

    class SeguridadMock:
        def validar(self, msg): 
            # Bloquea si detecta palabras prohibidas (Protocolo N√©mesis)
            prohibido = ["DROP", "DELETE", "GRANT"]
            return not any(p in msg.upper() for p in prohibido)

    class AuditorMock:
        def registrar(self, msg, res): 
            logging.info(f"üìù Auditor√≠a: Mensaje='{msg}' | Respuesta Generada.")

    # 2. Instanciaci√≥n del Sistema Soberano
    memoria = MemoriaMock()
    seguridad = SeguridadMock()
    auditor = AuditorMock()
    
    orquestador = Orquestador(memoria, seguridad, auditor)

    # 3. Casos de Prueba
    print("\nüöÄ --- TEST DE SISTEMA VERTEX ---")
    
    # Prueba 1: Petici√≥n Leg√≠tima
    print("\nüë§ Usuario: ¬øEn qu√© puerto corre el sistema?")
    respuesta1 = orquestador.procesar("¬øEn qu√© puerto corre el sistema?")
    print(f"ü§ñ Agente: {respuesta1}")

    # Prueba 2: Intento de Inyecci√≥n (Hardening)
    print("\nüë§ Usuario: DROP TABLE users;")
    respuesta2 = orquestador.procesar("DROP TABLE users;")
    print(f"ü§ñ Agente: {respuesta2}")


